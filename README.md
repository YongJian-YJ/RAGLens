# 🔍 RAGLens: Evaluate and Compare RAG Pipelines Visually

**RAGLens** is an interactive application for exploring, comparing, and evaluating Retrieval-Augmented Generation (RAG) pipelines. Built for researchers, developers, and learners, it provides visual insights into how different retrieval techniques, reranking strategies, and language models affect the quality and accuracy of generated answers.

## 🚀 Features

- 📚 **Upload your own documents & vector stores** to test RAG behavior
- 🔍 **Visualize context chunks** and how they influence LLM responses
- 🧠 **Evaluate accuracy** using G-Eval framework
- 🔁 **Compare retrievers**: BM25, Parent-Child, Hybrid, and more
- 🏷️ **Re-rankers supported**: BGE, MonoT5
- 🧪 **Experiment with chunking strategies** (size, overlap)
- 🔄 **LLM flexibility**: Use OpenAI, LLaMA 3, Qwen, DeepSeek, or local models via Ollama

## 🧪 How It Works

1. **Upload** a vector store or document.
2. **Choose** retrieval and reranking strategies.
3. **Enter a query** – RAGLens will fetch relevant contexts.
4. **Select a RAG Techniques** – response is generated and evaluated.
5. **Review metrics and visualizations** to understand performance.

## 🧠 Powered By

- [LangChain](https://www.langchain.com/)
- [Streamlit](https://streamlit.io/)
- [Ollama](https://ollama.com/)

## 👨‍🎓 Developed As Final Year Project

RAGLens was developed as part of the Final Year Project at Nanyang Technological University (NTU), focusing on improving transparency and usability of RAG pipelines for education and development purposes.
